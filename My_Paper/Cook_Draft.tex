\documentclass[12pt]{article}

\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{float}
\usepackage{titling}
\usepackage{amsmath} 
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage[colorlinks=true, urlcolor=blue]{hyperref}

\setlength{\droptitle}{-10em} 
\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}
%\addtolength{\textheight}{1.75in}
\makeatletter
\def\ps@pprintTitle{%
\let\@oddhead\@empty
\let\@evenhead\@empty
\def\@oddfoot{}%
\let\@evenfoot\@oddfot}
\makeatother


\begin{document}
	\title{A Network Perspective on LDA *DRAFT*}
	\author{Cindy Cook \\ \normalsize Network Analysis Final Project \\ \normalsize April 2015}
	\date{}
	\maketitle

\section{Introduction}

With an increase in the amount of digitized text, topic modeling has become an important area of research. One of the most popular methods for topic modeling is Latent Dirichlet Allocation (LDA) first described in \cite{lda}. LDA, a probabilistic hierarchical generative model, uses the Bayesian framework to discover hidden structure, or \textit{topics},  within a set of documents referred to as a corpus. Given a document, the distribution of latent topics is found through estimation of the posterior distribution. However, this is very challenging and computationally demanding since the posterior distribution of the latent variables given a document is intractable. A heavy development of fast and accurate approximation methods applied to LDA have been developed providing a user with several estimation options. A few of these options include collapsed Gibbs sampling (CGS) \cite{CGS}, collapsed variational Bayesian inference \cite{CVB}, maximum likelihood estimation \cite{ML}, and maximum a posteriori estimation \cite{MAP}. In \cite{lda}, a variational EM (VEM) algorithm is applied to solve the problem. Although the development of these methods have allowed researchers to get answers, a major problem lies in the fact that using different methods can often lead to conflicting results.
\\
\\
There have been several studies published comparing estimation procedures for LDA including \cite{BleiComp}, \cite{Comp}, \cite{CVB}, and \cite{Comp2}. Most comparison studies focus on a single corpus and evaluate different estimation methods using precision/recall statistics. In general, these papers offer good advice, but do not address the main problem of identifying inaccurate results. More recently, deeper theoretical problems with LDA have been highlighted by Lancichinetti in \cite{main}. The paper ``High-Reproducibility and High-Accuracy Method for Automated Topic Classification" shows that the likelihood of the true generating model can be smaller then the likelihood of alternative models in the LDA framework \cite{main}. After demonstrating their theoretical concerns, the authors tweak the LDA algorithm to provide a process they call TopicMapping, which leads to more robust results. TopicMapping involves five stages. The first is to clean the corpus, which involves stemming and removing of stop words. The second stage is to project the term frequency matrix onto the word space. The third stage filters the edges according to a Poisson distribution. The fourth stage is to apply InfoMap clustering to obtain starting values for each topic distribution over the words. The fifth and final stage is to apply LDA with these initial starting values. This process decreases the size of the parameter space, which aids in the estimation procedure. TopicMapping is tested on both simulated and real world corpus's.    
\\
\\
Although TopicMapping appears to solve the basic problems involved when applying LDA, I will show that it fails to produce both accurate and reproducible results across all types of corpus's. I hypothesis that basic differences in the modularity of the projected word network of a corpus determines whether or not TopicMapping will outperform LDA. The second section of this paper goes through a replication study done to ensure that the results of TopicMapping published in \cite{main} are accurate. It is also performed to make sure that I have the proper code and algorithms in place to apply their methods to other datasets. The third section introduces a new data set and explains how the networks used in this paper were obtained. The fourth section first applies TopicMapping to the new data and obtains neither accurate nor reproducible results. It then compares both the new and original corpus's using descriptive statistics. The fifth section uses ERGMs to analyze the differences between the two networks. The sixtha nd final section concludes the paper with a discussion of the results and future directions.

\section{Replicate the Study}

The paper, ``High-Reproducibility and High-Accuracy Method for Automated Topic Classification," consists of four main parts \cite{main}. The first points out a major limitation to the LDA algorithm developed by Blei as a topic model \cite{lda}. The second describes a new method called TopicMapping developed to overcome theoretical drawbacks of LDA. The third consists of comparing the accuracy and reproducibility of TopicMapping as compared to LDA and pLSA \cite{plsa} on synthetic data. The final aspect of the paper, applies TopicMapping to two real world datasets. I have been able to successfully reproduce the accuracy and reproducibility results for the comparisons between TopicMapping and randomly seeded LDA in both the simulated studies and one of the real world applications. The results of my replication can be found in Appendix A. All data and scripts can be found on github \href{https://github.com/cmcook22/Cook_Networks_Project}{here} .    

\section{The Data}

There are two main `real world' datasets used to test the TopicMapping algorithm:  Science and Wikipedia. The Science corpus contains $28,838$ documents from the web of science, where each document contains the title and abstract of a paper published in one of six top journals from different disciplines. After preprocessing the data, there are $106,143$ unique words and over $8.7 \times 10^{6}$ edges. The Wikipedia dataset contains over $4 \times 10^{6}$ nodes with over $800 \times 10^{6}$ edges. Due to the size of the Wikipedia data set, I was unable to receive this information from the authors of \cite{main}. Therefore, I will focus on the Science data set only. Although the Science corpus was easily sent to me, it is still considered `large.' In R, the adjacency matrix is too large to be stored. 
\\
\\
Due to the size of the WoS dataset, I randomly selected $10$ documents in each subject to create a working dataset with a total of $60$ documents. This dataset is called referred to as Science. The second corpus I will be considering was obtained from Indeed.com. Each document contains the past work experience from resumes within five different fields: teaching, graphic design, architecture, nursing, and accounting. The corpus contains $50$ documents. I will refer to this corpus as the resume corpus. 

\subsection{Creating the Networks}

After obtaining documents for both corpus's, the network of words is created in R by first using the \textit{tm} package to obtain the term frequency matrix. Then a stemmer is applied, and basic English stop words are removed. The \textit{bipartite} package is then used to project this term frequency (i.e. the adjacency matrix for a two-mode network) onto a one-mode network of words weighted by the number of times a pair of words appears together across all documents. Then the filtering process laid out in \cite{mainExtra} is applied. The following shows the size of the networks through this process:
\vspace{2mm}
\begin{center}
	\begin{tabular}{ |c|c|c||c|c|  }
		\hline
		&Science& &Indeed &\\
		\hline
		&Nodes&Edges&Nodes&Edges \\
		\hline
		original corpus& 2189& &1717 & \\
		cleaned corpus& 1995& & 1092&\\
		projected network& 1995& 121440& 1092&50534\\
		filtered network& 1995&121039 & 1092&50186\\
		\hline
	\end{tabular}
\end{center}
 
\section{Comparison}
After applying TopicMapping to both Science and Indeed, I obtained the following results: 
\vspace{2mm}
\begin{center}
	\includegraphics[scale=0.5]{Images/comparison1.jpeg}
\end{center} 
\vspace{2mm} 
We can see that TopicMapping performed with approximately $70\%$ accuracy and $70\%$ reproducibility for the Science corpus. However, the accuracy is around $10\%$ and the reproducibility is around $100\%$ for the Indeed corpus. There is clearly a wide range in the accuracy of TopicMapping. I hypothesize that TopicMapping is highly dependent on the clustering done by InfoMap. If InfoMap does not produce accurate results, it can lead the estimation procedure of LDA into the wrong part of the parameter space leading to completely inaccurate results. The major difference between the two corpus's lies in the fact that words used in titles and abstracts of journal articles do not readily appear across different disciplines. Whereas words used in resumes to describe past work experience cross fields more easily. In fact, we can see from the following plot, the proportion of words from our two cleaned corpus's that cross these boundaries:
\begin{center}
	\includegraphics[scale=0.5]{Images/props.jpeg}
\end{center} 
\subsection{Descriptive Statistics}

To begin analysis on these networks, I use the \textit{igraph} package in R to measure the betweenness centrality for each node, which will allow me to analyze structural holes and bridges. Words that are highly used within each document and common across all documents will have high betweenness scores. As we see in the plot above less then $1\%$ of the words in the science corpus appear in at least one document from all six disciplines. Whereas, about $4\%$ appear in at least one resume from all  
\vspace{2mm}
\begin{figure}[h]
	\hfill
	\subfigure[Science]{\includegraphics[scale=0.5]{Images/Sci_between.pdf}}
	\hfill
	\subfigure[Indeed]{\includegraphics[scale=0.5]{Images/Indeed_between.pdf}}
	\hfill
	\caption{Betweenness Centrality}
\end{figure}
\vspace{2mm}

Words with the highest betweenness scores for Science are as follows:  result (123287.95), use (67756.22), model (67159.62), data (60749.09), suggest (56224.20), and studi (55947.48). Words with the highest betweenness scores for Indeed are as follows:  present (39422.6), pa (31815.2), respons (19401.64), includ (18680.75), work (16158.91), and design (12703.92). As predicted, these words are used often in each document no matter what discipline or field the document is in. This fact would be obvious for any projected word network. There is only a slight difference in the distribution of betweenness scores. We find that for the Science corpus only $680$ words in the network $(34\%)$, have a betweenness centrality greater than one, while $1829$ $(92\%)$have a betweenness score greater than $0$. For the Indeed corpus, $404$ words $(37\%)$ of the words have a betweenness score greater then 1, while $1061$ or $(98\%)$ of the words have a betweenness score greater than 0. The concept of TopicMapping is more successful in the Science network due to the fact that words from one topic (i.e. one journal) tend to cluster, while fewer words tend to act as bridges between these clusters. Whereas we see the same phenomea in the the Indeed network, just at a higher rate.
\\
\\
Next, I will find the transitivity score of the network. Due to the clustering of the nodes, I hypothesis that this value is positive and high. In fact it is $0.4181239$ for the Science corpus and $0.4279717$ for the Indeed corpus. These values are really close indicating that the number of transitive triads is very similar in both networks. In fact we can see the following triad censuses:
\vspace{2mm}
\begin{center}
	\begin{tabular}{ |c||c|c|c|c|  }
		\hline
		&003&102&201&300 \\ 
		\hline 
		Science& $83.40\%$ & $15.21\%$ & $1.12\%$ & $0.27\%$ \\
		Indeed& $77.78\%$ & $19.67\%$ & $2.04\%$ & $0.51\%$ \\
		\hline
	\end{tabular}
\end{center}
\vspace{2mm}
that first there are only four possible triads, and the distribution of these are very similar in both Science and Indeed. 
\section{ERGMS}

Since my hypothesis is that projected word networks which do not cluster well will not perform well in terms of TopicMapping, I plan to explore the extent of homophily in each network as quantified using ERGMS. For each network, I first use the InfoMap clustering algorithm in the \textit{iGraph} package in R to obtain membership ids for each word. I then fit an ergm with the nodematch term to test for homophily in each network. I include both edges and isolates in the network. I obtain the following Maximum Pseudolikelihood Results:
\vspace{2mm}
\begin{center}
	\begin{tabular}{ |c|c|c|c|c||c|c|c|c|c|c|  }
		\hline
		Science&&&&&Indeed&&& \\
		\hline
		&Estimate&Std. Error&MCMC$\%$&p-value&Estimate&Std. Error&MCMC$\%$&p-value \\ 
		\hline 
		edges&-3.35858&0.004053&0&$<0.0001$&-2.986225&0.006292&0&$<0.0001$\\
		isolate&-Inf&0&0&$<0.0001$&-Inf&0&0&$<0.0001$\\
		edges&3.373386&0.007151&0&$<0.0001$&3.159993&0.011496&0&$<0.0001$\\
		\hline
	\end{tabular}
\end{center}
\vspace{2mm}
First note that the MCMC percentages are all 0 since I am using MPLE estimation and not MLE. Also note that the coefficient for the isolates term is negative infinity since there are no isolates in either network. I included the term because by definition, there should never be any isolate in a word projection network. Next we should notice that both of the homophily terms are significant, positive, and approximately $3$. There is a slight decrease in the term for Indeed, but not as much as I expected. It is important before analyzing these results further to look at goodness of fit statistics. The plots can be found in Appendix B. We can clearly see that neither ergm fits well. In fact, both ergms appear to be degenerate. After attempting to include different degree properties, edge weights, and several other options, none of the more complex models converge. To verify these results, I fit QAP models, where the response is the weighted projected word network, and the predictor is the binary network indicating if two nodes are connected and if they are in the cluster. I obtain the following results:
\vspace{2mm}
\begin{center}
	\begin{tabular}{ |c|c|c|c|c||c|c|c|c|c|c|  }
		\hline
		Science&&&&&Indeed&&& \\
		\hline
		&Estimate&Pr$(<=b)$ &Pr$(>=b)$ &Pr$(>=|b|)$&Estimate&Pr$(<=b)$ &Pr$(>=b)$ &Pr$(>=|b|)$ \\ 
		\hline 
		intercept&0.5906065& 1&       0&       0&0.5423806& 1&       0&       0&      \\
		x1&-0.5517617 &0       &1       &0&-0.4856092 &0       &1       &0\\
		\hline
	\end{tabular}
\end{center}
\vspace{2mm}
Note that for the binary predictor, a one represents that two documents are not in the same cluster. The Science QAP has an adjusted $R^2$ of 0.1757, while Indeed has an adjusted $R^2$ of 0.1386.As we can see, the two models are extremely similar, which is not what I hypothesized. However, I will note that although the coefficient is negative, we still have the same interpretation as with the ergms. A new additional node with a cluster id will attach to other nodes with the same clustering id with a coefficient of 0.59 on average and will attach to a node with different clustering ids with a coefficient of 0.04 for Science and coefficients of 0.54 and 0.06 for Indeed respectfully.    
\section{Conclusions}

\subsection{Discussion}

\subsection{Future Directions}






\newpage
\section{Bibliography}
\begin{thebibliography}{4}
		
	\bibitem{STACK}
	Anonymous. (2012),
	``Two R Packages for Topic Modeling, lda and topicmodels?" \textit{Cross Validated}: http://stats.stackexchange.com/questions/24441/two-r-packages-for-topic-modeling-lda-and-topicmodels.
	
	\bibitem{Comp} 
	Asuncion, A., Welling, M., Smyth, P., and Teh, Y.  (2009),
	``On Smoothing and Inference for Topic Models." 
	\textit{UAI}: 27-34.
	
	\bibitem{plsa}
	BHoffman, T.  (1999),
	``Probabilistic Latent Semantic Indexing."
	\textit{Proccedings of the Twenty-Second Annual International SIGIR Conferance}.
		
	\bibitem{lda}
	Blei, D., Ng, A., and Jordan, M.  (2003),
	``Latent Dirichlet Allocation."
	\textit{Journal of Machine Learning Research}: 3 993-1022.
	
	\bibitem{Ccode}
	Blei, D. (2004),``LDA-C."
		
	\bibitem{ldaR}
	Chang, J.  (2015),
	``Package `lda'."
	\textit{CRAN}.

	\bibitem{MAP}
	Chien, J., and Wu, M.  (2008),
	``Adaptive Bayesian Semantic Analysis."
	\textit{Audio, Speech, and Language Processing}, IEEE Transactions on: 16(1), 198-207.

	\bibitem{CGS}
	Griffiths, L. and Steyvers, M.  (2004),
	``Finding Scientific Topics."
	\textit{PNAS}: 1(Suppl 1), 5228-5235.

	\bibitem{topicsR}
	Grun, B. and Hornik, K.  (2015),
	``Package `topicmodels'."
	\textit{CRAN}.
	
	\bibitem{ML}
	Hofmann, T.  (2001),
	``Unsupervised Learning by probabilistic Latent Semantic Analysis."
	\textit{Machine Learning}: 42(1), 177-196.
	
	\bibitem{main}
	Lancichinetti, A., Sirer, M., Wang, J., Acuna, D., Kording, K., and Amaral, Luis. (2015),
	``High-Reproducibility and High-Accuracy Method for Automated Topic Classification."
	\textit{Physical Review X}: 5, 0011007, 2160-3308.
	
	\bibitem{mainExtra}
	Lancichinetti, A., Sirer, M., Wang, J., Acuna, D., Kording, K., and Amaral, Luis. (2015),
	``High-Reproducibility and High-Accuracy Method for Automated Topic Classification: Supplemental Material."
	\textit{Physical Review X}: 5, 0011007, 2160-3308.
	
	\bibitem{BleiComp} 
	Mukherjee, I. and Blei, D.  (2009),
	``Relative Performance Grantees for Approximate Inference in Latent Dirichlet Allocation." 
	\textit{NIPS}: 21, 1129â€“1136.
	
	\bibitem{Comp2}
	Speh, J., Muhic, A., and Rupnik, J.  (2013),
	``Parameter Estimation for Latent Dirichlet Allocation." \textit{Artificial Intelligence Laboratory}: Ljubljana, Slovenia.
		
	\bibitem{CVB}
	Teh, Y. W., Newman, D., and Welling, M.  (2007),
	``A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet 
	Allocation."\textit{NIPS}: 3, 1353-1360.
	
\end{thebibliography}

\newpage
\appendix
\section{Replication Study}
\subsection{LDA Limitations}
The paper \cite{main} begins with a simple thought experiment. Given a Corpus consisting of documents in three distinct languages, say English, Spanish, and French, let all words in each language be unique and each document contain only one language. Ideally, a topic model would produce three distinct topics, one for each of the unique languages. However, this is not the case with LDA. The authors of \cite{main} show the likelihood of the generative model (i.e. placing all English documents in one topic, all Spanish documents in another, and all French documents in the last topic) is not always maximum. In fact they describe an alternative model, which theoretically obtains a higher likelihood. The alternative model is defined by separating one language into two dialect topics and combining the other two languages into one topic. In fact the authors prove in the supplemental material that, ``it is possible to find an extremely large number of alternative models (with the same number of topics) which overfit some topics and underfit some others but have a better likelihood then the true generative model" \cite{mainExtra}. These results are visualized in Fig. 2 (pg. 3,\cite{main}). Part a illustrates the differences in the generative and alternative models considered. Part b and c show the theoretical limits on the log likelihood for both generative and alternative models when considering a vocabulary of $60$ unique words, where $20$ are in either $E, S,$ or $F$. The corpus contains $1000$ documents, where each document contains $10$ words chosen uniformly from one language. Specifically, they show that the for these particular parameter values, if the fraction of $E$ documents in the corpus is greater than $0.936,$ then the likelihood of the alternative model become greater than the likelihood for the generative model. 
\\

\noindent In order to replicate part d of Fig. 2, I will first create a synthetic language corpus and then apply the LDA algorithm as programed in the \textit{topicmodels} package in R. We can determine the success rate as shown in Fig. 2 (pg. 3 \cite{main}) part d. Here, I also consider the average cosine similarity between each document's topic distribution and the true topic distribution. Following the process in \cite{main}, I first create a list of twenty words in English $E$, Spanish $S$, and French $F$. I then create a corpus of $1000$ documents, where the number of $E, S,$ and $F$ documents follow the following equation:
$$
100=pE+\frac{(100-p)}{2}S+\frac{(100-p)}{2}F,
$$ 
for $p=0.5, 0.6, 0.7, 0.8, 0.9, 0.96$. Note that each of the twenty words of a particular language has an equal probability of being sampled for each document with replacement, and each document consists of exactly ten words. I then apply LDA using the Variational EM Approach, which was used in \cite{main}. I run the algorithm $100$ times calculating the average cosine similarity and success rate with each run to obtain the following results:   
\vspace{2mm}
\begin{center}
	\begin{tabular}{ ||c|c|c|c|c|c|c||  }
		\hline
		$p$ &0.5&0.6&0.7&0.8&0.9&0.96 \\ 
		\hline 
		Min.    &0.5763 &0.5593 &0.5318 &0.5780 &0.5774 &0.5765\\
		1st Qu. &0.5844 &0.5821 &0.5802 &0.5800 &0.5791 &0.5774\\
		Median  &1.0000 &0.5871 &0.5834 &0.5822 &0.5802 &0.5777\\
		Mean    &0.8252 &0.7240 &0.6481 &0.5919 &0.5803 &0.5776\\
		3rd Qu. &1.0000 &1.0000 &0.5913 &0.5852 &0.5809 &0.5779\\
		Max.    &1.0000 &1.0000 &1.0000 &1.0000 &0.5858 &0.5783\\
		\hline
	\end{tabular}
\end{center}
\vspace{2mm}

\noindent We Notice that the minimum cosine similarity stays relatively constant across percent levels. This is due to the fact that in all runs, there are only three options of outcomes:  First, LDA correctly identifies the generative model resulting in a cosine similarity measure of about $0.99$; Second, LDA identifies the alternative model resulting in a cosine similarity of about $0.73$; and Third, LDA assigns a uniform topic distribution for each document resulting in a cosine similarity of about $0.55$. Although LDA applies a uniform topic distribution each of the corpuses at least once, we notice that when the percent level is $0.9$ and $0.96$ the algorithm applies a uniform topic distribution in all $100$ runs. Also, notice that the average cosine similarity decreases as the percent of English documents increases as expected. The following plot shows the success rate as the percentage of runs that correctly identifies the generative model:  
\begin{center}
	\includegraphics[scale=0.5]{Images/plot_smallC.jpeg}
\end{center} 
\vspace{2mm}
We now notice that the success rate is lower than the paper suggests, but there are many reasons for this. First, we are not analyzing the same corpus. Second, LDA uses Variational EM methods to fit the topic distribution, which is a stochastic process. Even though the overall success rate is lower, the trend stays the same. LDA tends to produce the correct generative model when the percentage of English documents is lower. In this example, when the percentage of English documents is greater than $0.9$, LDA never produces the generative model.

\subsection{TopicMapping}

The second part of the paper introduces a new algorithm for topic modeling called TopicMapping. The algorithm is described in \cite{main}, with more details in \cite{mainExtra}. In order to reproduce the article's main empirical results, I need to run their algorithm. I contacted the author's to not only obtain their original data, but their code if possible. I received code from one of the authors. The code is written mostly in C++, but also in Python. After several attempts, I was able to successfully run their code on a mac, but have run into the following error on a pc:  ``InfoMap did not compile. Please contact me: arg.lanci@gmail.com ." I have sent an email asking for help, but have yet to hear back. I was also able to code the algorithm in R. However, my code is neither efficient nor fast. For the following results, I ran everything I could on my pc, but decided to simply run the TopicMapping algorithm on a mac, save the results to a flash drive, and upload them to my pc for analysis. 

\subsection{Comparison of LDA to TopicMapping}

There are two types of major synthetic data analyzed in the paper. The first type of data is similar to that in Section 2.1, with corpuses of differing sizes contain ten languages in different proportions. The other type of data consists of corpuses constructed using the generative process defined in \cite{lda}, where the number of documents is $1000,$ the number of words in each document is $50$, the number of topics is set to $20$, and the hyper-parameters are set at $\alpha=0.001,0.004,0.016,0.064$. They also vary the percentage of words in the corpus that are considered generic, or equally likely across topics. Along with considering cases where each topic is equally distributed across documents, they consider cases where four topics make up $15\%$ and the last $16$ topics make up $2.5\%$. To simplify the amount of computing, I will focus on six corpuses of the first type:  Three corpuses have ten equally distributed languages of size $1000, 5000, 10000,$ and the other three have two languages making up $15\%$ each of the corpus and the remaining eight languages making up $8.75\%$ of size $1000, 5000, 10000$. I will also analyze twelve corpuses of the second type:  Six corpuses have $\alpha=0.001$ with generic proportion $0.2, 0.5, 0.8,$ and either equal topic distribution or not; the last six have $\alpha=0.064$ with generic proportion $0.2, 0.5, 0.8$, and either equal topic distribution or not.
\\

\noindent I was able to compile the code given to me by the authors, which simulate similar, but not the exact synthetic datasets. They did not save their original seed values. I was also able to successfully compile their program, which calculates the accuracy and reproducibility of a model. My aim is to reproduce the results as shown in Fig. 4 (pg.5, \cite{main}) and Fig. 7 (pg.6, \cite{main}) using a mac to run the TopicMapping algorithm and my pc to run LDA in R. The two other models that are considered by the authors are pLSA and LDA with seeded start values. Since the authors do not provide an explanation of what the start seeds are or how to find them, I ignore this model comparison. I also ignore the comparison to pLSA since this algorithm is not already programed in R nor do the authors provide code for it. The results for LDA and pLSA were similar to the results of LDA in all cases studies considered. Therefore, I will focus on reproducing the results for comparisons between TopicMapping and randomly seeded symmetric LDA only.
\\

\noindent I begin with the language corpuses, where I obtained the following accuracy and reproducibility results:
\vspace{2mm}
\begin{center}
	\includegraphics[scale=0.8]{Images/plot_largeC.jpeg}
\end{center} 
\vspace{2mm}
\noindent The plot above shows accuracy (top) and reproducibility (bottom) results for running LDA (green) and TopicMapping (blue) $100$ times on both equal (left) and unequal (right) topic distributions. With both algorithms, the number of topics was fixed at $10$. The median for the $100$ runs is plotted, where dotted lines indicate the $25$th and $75$th percentiles for both algorithms as described in \cite{main}. We can clearly see that the results follow the same patterns as shown in \cite{main}. TopicMapping is performing at near perfect accuracy and reproducibility for both equally and unequally sized topics regardless of the number of documents in the corpus. LDA clearly performs with greater variance, and does slightly better as the number of documents increases. Yet, TopicMapping outperforms LDA in all cases.
\\

\noindent The next results we will look at are those obtained from running both LDA and TopicMapping on the second set of simulated datasets when the hyper-parameters are set to $0.001$:
\vspace{2mm}
\begin{center}
	\includegraphics[scale=0.8]{Images/plot_Alpha1.jpeg}
\end{center} 
\vspace{2mm}

\noindent The figure above shows accuracy (top) and reproducibility (bottom) results for running LDA (green) and TopicMapping (blue) $100$ times on both equal (left) and unequal (right) topic distributions. With both algorithms, the number of topics was fixed at $20$ and the hyper-parameters are set at $0.001$. Again, the median for $100$ runs is plotted, where dotted lines indicate the $25$th and $75$th percentiles for both algorithms. We can clearly see that the results follow the same patterns as shown in \cite{main}. TopicMapping is performing at near perfect accuracy and reproducibility for both equally and unequally sized topics regardless of the number of documents in the corpus. LDA clearly performs with greater variance, and does slightly better as the percentage of generic words decreases. Yet, TopicMapping outperforms LDA in all cases.
\\

\noindent Lastly, we will look at the results obtained from running both LDA and TopicMapping on the second set of simulated datasets when the hyper-parameters are set to $0.064$:
\vspace{2mm}
\begin{center}
	\includegraphics[scale=0.8]{Images/plot_Alpha2.jpeg}
\end{center} 
\vspace{2mm}

\noindent The figure above shows accuracy (top) and reproducibility (bottom) results for running LDA (green) and TopicMapping (blue) $100$ times on both equal (left) and unequal (right) topic distributions. With both algorithms, the number of topics was fixed at $20$ and the hyper-parameters are set at $0.064$. Again, the median for $100$ runs is plotted, where dotted lines indicate the $25$th and $75$th percentiles for both algorithms. We can clearly see that the results follow similar patterns as shown in \cite{main}. Both LDA and TopicMapping perform uniformly across differing percentages of generic words. TopicMapping with $0.7$ accuracy and $0.8$ reproducibility outperforms LDA, which performs poorly. Although LDA consistently performs poorly with unequally sized topics, TopicMapping's accuracy and reproducibility drop as the percentage of generic words increases, which is exactly what we see in \cite{main}.


\subsection{Applications}

I received emails back from two of the authors providing me with the corpus for the first application dataset:  \textit{Web of Science} or WoS. Both authors stated that the second application dataset, Wikipedia, was too large to send. They stated that even if they provided me access to it through some means, I would be unable to successfully reproduce their results in a semester. I will focus on reproducing the results for the first application only. The WoS Corpus contains documents consisting of the titles and abstracts for $23838$ journal articles published in six different top journals for the following fields:  astronomy, biology, economics, geology, mathematics, and psychology. I was able to reproduce the results on Fig. 8 (pg. 7, \cite{main}):

\vspace{2mm}
\begin{center} 
	\includegraphics[scale=0.5]{Images/plot_WoS.jpeg} 
\end{center} 
\vspace{2mm}  

\noindent I have plotted the median accuracy (left) and reproducibility (right) from the $100$ runs for both TopicMapping (blue) and LDA (green) with dotted lines representing the $25$th and $75$th percentiles. The results were found when the number of topics was set at $6$ the correct number and at $24$ as in \cite{main}. We notice that with both cases the correct number of topics and incorrect number, TopicMapping does well in both accuracy and reproducibility. However, LDA performs better when the correct number of topics specified. Again, TopicMapping outperforms LDA in all cases. 

\section{ERGM GOF}
We can see in the plots below show how complex the network is by the fact that the images do not fit nicely on this page:
\vspace{2mm}
\begin{center} 
	\includegraphics[scale=0.05]{Images/Sci_fit2.pdf} 
	\includegraphics[scale=0.05]{Images/Indeed_fit2.pdf} 
\end{center} 
\vspace{2mm}
The following show the first part of each image blown up. We can see that neither ergm model fits well:
\vspace{2mm}
\begin{center} 
	\includegraphics[scale=0.5]{Images/Sci_fit2.pdf} 
	\includegraphics[scale=0.5]{Images/Indeed_fit2.pdf} 
\end{center} 
\vspace{2mm}
\section{Extra Stuff for now}
Lastly, I found that the degree assortitivity is negative and low at $ -0.06119152$ for the Science corpus. It is also negative for the Indeed corpus, but is twice as large at  $-0.12577$. Here we see that words which are highly connected do not tend to connect with each other. This is more so for Indeed then for Science. In terms of a projected word network, this means that words such as (what are these words??) tend to 
\par
In particular, the program crashes while trying to plot the network using the plot function in the network package. In order to visually see the network, I used only the first $10$ documents to create a network.

\begin{figure}[h!]
	\centering
	\includegraphics[width=70mm,height=60mm]{Images/simple_plot.pdf}
	\caption{Network Plot of the first $10$ Documents} 
\end{figure} 

What we immediatley see in this plot are very distinct clusters along with very distinct bridges. Explain what this is informing us here...
The article also makes use of several synthetic datasets: list and explain here for what I will use...





\end{document}